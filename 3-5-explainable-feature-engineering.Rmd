## Interpretable, non-linear feature engineering techniques for linear regression models - exploration on concrete compressive strength dataset with a new feature importance metric.

*Authors: Łukasz Brzozowski, Wojciech Kretowicz, Kacper Siemaszko (Warsaw University of Technology)*

### Abstract
In this article we present and compare a number of interpretable, non-linear feature engineering techniques used to improve linear regression models performance on Cocrete compressive strength dataset. To assert their interpretability, we introduce a new metric for measuring feature importance, which uses derivatives of feature transformations to trace back original features' impact. As a result, we obtain a thorough comparison of transformation techniques on two black-box models - Random Forest and Support Vector Machine -  and three glass-box models - Decision Tree, Elastic Net, and linear regression - with the focus on the linear regression.

### Introduction and Motivation

Linear regression is one of the simplest and easiest to interpret of the predictive models. While it has already been thorougly analysed over the years, there remain some unsolved questions. One such question is how to transform the data features in order to maximize the model's effectiveness in predicting the new data. 

An example of a known and widely used approach is the Box-Cox transformation of the target variable, which allows one to improve the model's performance with minimal increase in computational complexity [@boxcox]. However, the choice of the predictive features' transformations is often left to intuition and trial-and-error approach. In the article, we wish to compare various methods of features' transformations and compare the resulting models' performances while also comparing their computational complexities and differences in feature importance. 

Many black box regression models use various kinds of feature engineering during the training process. Unfortunately, even though the models perform better than the interpretable ones, they do not provide information about the transformations used and non-linear dependencies between variables and the target. The goal we want to achieve is extracting features and non-linearities with understandable transformations of the training dataset. 

To measure the improvement of used methods we will compare their performance metrics with black box models' as a ground truth. This will allow us to effectively measure which method brought the simple linear model closer to the black box. Moreover, we will take under consideration the improvement of black box model performance. Thanks to this, our article will not only present the methods for creating highly performant interpretable models, but also improvement of the results of black box model.

### Related Work
There exist many papers related to feature engineering. We will shortly present two of them.

One of these papers is "Enhancing Regression Models for Complex Systems Using Evolutionary Techniques for Feature Engineering" Patricia Arroba, José L. Risco-Martín, Marina Zapater, José M. Moya & José L. Ayala [@ETFFE]. This paper describes, how feature transformations in linear regression can be chosen based on the genetic algorithms.

Another one is "Automatic feature engineering for regression models with machine learning: An evolutionary computation and statistics hybrid" Vinícius Veloso de Melo, Wolfgang Banzhaf [@VELOSODEMELO]. Similarly to the previous one, this paper tries to automate feature engineering using evolutionar computation to make a hybrid model - final model is simple linear regression while its features are found by more complex algorithm.

### Methodology

The main goal of our research is to compare various methods of transforming the data in order to improve the linear regression's performance. While we do not aim to derive an explicitly best solution to the problem, we wish to compare some known approaches and propose new ones, simultaneously verifying legitimacy of their usage. The second goal of the research is to compare the achieved models' performances with black box models to generally compare their effectiveness. An important observation  about the linear regression model is that once we transform features in order to improve the model's performance, it strongly affects its interpretability. We therefore propose a new feature importance measure for linear regression and compare it with the usual methodes where possible.

The four methods of feature transformation compared in the article include:

1. By-hand-transformations - through a trial-and-error approach we derive feature transformations that allow the linear regression models to yield better results. We use our expertise and experience with previous datasets to get possibly best transformations which are available though such process. They include taking the features to up to third power, taking logarithm, sinus, cosinus or square root of the features.

2. Brute Force method - this method of data transformation generates huge amount of additional features being transformations of the existing features. We allow taking each feature up to the third power, taking sinus and cosinus of each feature and additionaly a product of each pair of features. The resulting dataset has $69$ variables including the target (in comparison with $9$ variables in the beginning).

3. Bayesian Optimization method [@BO] - we treat the task of finding optimal data transformation as an optimization problem. We restrict the number of transformations and their type - we create one additional feature for each feature present in the dataset being its $x$-th power, where $x$ is calculated through the process of Bayesian optimization. It allows us to keep the dimension of the dataset relatively small while improving the model's performance. We allowed the exponents to be chosen from interval $(1, 4]$.

4. One of our ideas is to use GP (Genetic Programming) to find best feature transformations. Our goal is to find simple feature transformations (not necessarily with arity 1) that will significantly boost oridnary linear regression. We will create a set of simple operations such as adding, multiplying, taking a specific power, for example 2, taking logarithm and so on. Each transformation is based only on these operations and specified input variables. Each genetic program tries to minimize MSE of predicting target, thus trying to save as much information as possible in a single output variable constructed on a subset of all features. We will use one of the variations of the genetic algorithms to create an operation tree minimizing our goal. Before we fit final model, that is orinary linear regression, first we select a proper transformed variables. Final linear regression, is calculated on the transformed and selected variables at the very end. Summarizing, each operation tree calculates one variable, but this variable is not guaranteed to be included at the end. To avoid strong corelation we decided to use similar trick to one used in random forest. Each GP is trained only on some subset of all variables. Using boostrap is also a good idea, though we did not test it. If data has got low dimensionality, then you can consider using all possible subsets of variables of fixed size. Otherwise, you can randomly choose them. This process results in a many output variables, so we choose final transformed features using LASSO regression or BIC. This method should find much better solutions without extending dimensionality too much or making too complex transformations. We will get linear regression with much better performance without loss of the interpretability. Whole conception tries to automate feature enginnering done traditionally by hand. Another advantage is control of model complexity. We can stimulate how the operation trees are made, for instance, how the operations set looks like. There is also a possibilty of reducing or increasing complexity at will. Modification of this idea is to add regularization term decreasing survival probability with increasing complexity. At the end model could also make a feature selection in the same way - then one of possible operations in the set would be dropping.

The research is conducted on *Concrete_Data* dataset from the OpenML database [[link](https://www.openml.org/d/4353)]. The data describes the amount of ingredients in the samples - cement, blast furnace slag, fly ash, water, coarse aggregate and fine aggregate - in kilograms per cubic meter; it also contains the drying time of the samples in days, referred to as age. The target variable of the dataset is the compressive strength of each sample in megapascals (MPa), therefore rendering the task to be regressive. The dataset contains 1030 instances with no missing values. There are also no symbolic features, as we aim to investigate continuous transformations of the data. Due to the fact, that we focus on the linear regression model, the data is reduced prior to training. We remove the outliers and influential observarions based on Cook's distances and standardized residuals.

We use standard and verified methods to compare results of the models. As the target variable is continuous, we may calculate Mean Square Error (MSE), Mean Absolute Error (MAE), and R-squared measures for each model, which provide us with proper and measurable way to compare the models' performances. The same measures may be applied to black box models. The most natural measure of feature importance for linear regression are the coefficients' absolute values after training the model - however, such easily interpretable measures are not available for black-box models. We therefore measure their feature importance  with permutational feature importance measure and caluclating drop-out loss, easily applicable to any predictive model and therefore not constraining us to choose from a restricted set. In order to provide unbiased results, we calculate the measures' values during cross-validation process for each model, using various number of fold to present comparative results. 

### Results

 <center>
 ```{r mae3-5, echo=FALSE, fig.cap="MAE of the models after transformations", out.width = '70%'}
knitr::include_graphics("images/3-5-1.png")
```
 ```{r mse3-5, echo=FALSE, fig.cap="MSE of the models after transformations", out.width = '70%'}
knitr::include_graphics("images/3-5-2.png")
```

</center>

The plots presented above show the values of MAE and MSE achieved by each model on the datasets after the mentioned transformation. We may observe, that linear models have significantly reduced their error values after the transformations, while the brute force method yielded best results. However, the Bayesian optimization, trial-and-error method and genetic modifications also provided much improvement in comparison with the models' performance on the plain dataset. The linear models were finally able to achieve results comparable to SVM performance on the unchanged data.

### Summary and conclusions 

